---
title: "Problem set 3"
author: "Yonatan Schoen"
date: "2023-05-28"
output:
  html_document: default
  pdf_document: default
---

# ML for economics - Problem Set 3

### Packages install
```{r setup, warning= FALSE, message= FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  tidyverse,   # for data wrangling and visualization
  broom,       # for tidying estimation output
  here,        # for referencing folders and files
  glmnet,      # for estimating lasso and ridge
  DataExplorer,
  tidymodels,
  parsnip,
  caret,
  rsample,
  yardstick,
  caTools,
  pROC,
  rpart.plot,  
  ranger,
  rsample,
  vip,        
  knitr,       
  here,        
  rattle,
  magrittr,
  caret,
  DALEX,
  RColorBrewer,
  ada,
  doParallel,
  pROC,
  tinytex
)
```

# Data upload and splitting
```{r, message=FALSE}
df <- read_csv("kaggle_train.csv") 
real_test <- read_csv("kaggle_test.csv")

# Split our data to train and test
set.seed(42)
split <- df %>% 
            initial_split(prop = 0.7)

train <- training(split)
test <- testing(split)

```

# Data exploration
```{r}
train %>% summarize()
```
The first thing that we can see is that the `farmer` indicator equal 0 for every obseravtions.

## We would like to start with some data wrangling :
1. We'll drop the `farmer` variable since all of the observations recieve 0 for it.  
```{r}
train <- train %>% select(-farmer)
```


Now we want to look at the data and check if we can infere some meaningful correlations or identify patterns we can use for prediction 

## Experience variables
```{r}
# Full time experience 
train %>% ggplot(aes(x = expf, y = lnwage)) +
  geom_jitter() +
  geom_smooth()

# Full time experience squared
train %>% ggplot(aes(x = expfsq, y = lnwage)) +
  geom_jitter() +
  geom_smooth()

# Part time experience
train %>% ggplot(aes(x = expp, y = lnwage)) +
  geom_jitter() +
  geom_smooth()

# Part time experience squared
train %>% ggplot(aes(x = exppsq, y = lnwage)) +
  geom_jitter() +
  geom_smooth()
```

We can conclude that expect from part time squared experience all other variables seem relevant to prediction.
In addition, it's evident that there is strong correlation of the exprience level especially in the early stages of the career.   

Therefore we decided to drop ```exppsq``` from over model 


# Model testing 

# Build the model recipe :
```{r}
cv_folds <- train %>%
  vfold_cv(v = 5) 

#recipe and model plan
recipe_fit <- recipe(lnwage ~ ., data = train %>% select(-ID)) %>%
  step_mutate(
    grad_hs  = if_else(edyrs >= 12, 1, 0), 
    grad_uni = if_else(edyrs >= 16, 1, 0), 
    skilled  = if_else(
      production == 1 | transport == 1 | constructextractinstall == 1 | 
      foodcare == 1 | healthsupport == 1 | building == 1 | 
      sales == 1 | officeadmin == 1, 0, 1)
  ) %>% 
  step_scale(all_numeric_predictors())
```



## Ridge
```{r}
#create the X and Y data set
train_mat <- train %>% as.matrix()

Y <- train_mat[,2]
X <- train_mat[, 3:38] 

#fit the model
fit_ridge <- glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# Plotting for illustration
plot(fit_ridge, xvar = "lambda")


# Cross validation for the ridge model
cv_ridge <- cv.glmnet(x = X, y = Y, alpha = 0)
plot(cv_ridge)
```
## Lasso
```{r}
#fit the model
fit_lasso <- glmnet(
  x = X,
  y = Y,
  alpha = 1
)

# Plotting for illustration
plot(fit_lasso, xvar = "lambda")

# Cross validation for the lasso model
cv_lasso <- cv.glmnet(x = X, y = Y, alpha = 1)
plot(cv_lasso)

```
```{r}
# Keep the coeficients
lasso_coef <- coef(cv_lasso, s = "lambda.1se") 
ridge_coef <- coef(cv_ridge, s = "lambda.1se") 

# look at the model
lasso_coef
```

The lasso model identified a number of important variables for prediction.  We now proceed to using them in a random forests framework. 

Our prediction plan is as follow :
1. lasso prediction 
2. Random Forest
(3. gbm ?)
4. Random forest + variable selection using lasso
5. comparison of the RMSE on our own test set
6. prediction uploaded to Kaggle

## Random Forest - full formula
```{r}
tree_fit <- rpart(
  formula = lnwage ~ ., 
  data = train,
  method = "class"
  )

tree_prune <- prune(tree_fit, cp = 0.03)

rpart.plot(tree_prune)
```
## gbm - boosting
```{r}
avg = 

```